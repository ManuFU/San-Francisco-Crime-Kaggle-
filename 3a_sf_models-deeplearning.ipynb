{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libareries\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.layers import LSTM, SimpleRNN, Dense, PReLU, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from sklearn import model_selection\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "#from cm import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframes from preproc\n",
    "x_train = pd.read_pickle('x_Train')\n",
    "x_test = pd.read_pickle('x_test')\n",
    "y_test = pd.read_pickle('y_test')\n",
    "y_train = pd.read_pickle('y_train')\n",
    "test_data = pd.read_pickle('test_data')\n",
    "\n",
    "labels = np.array(['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY',\n",
    "       'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE',\n",
    "       'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION',\n",
    "       'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING',\n",
    "       'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING',\n",
    "       'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES',\n",
    "       'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE',\n",
    "       'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE',\n",
    "       'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE',\n",
    "       'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT',\n",
    "       'WARRANTS', 'WEAPON LAWS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelToCsv_deeplearning(model):\n",
    "    \"\"\"Die Methode schreibt predictions des übergebenen Modells in eine CSV Datei Namens 'submit.csv'.\n",
    "    Die CSV Datei hat das von Kaggle gewünschte Format.\"\"\"\n",
    "    #Droppe Column Id, um die gleichen Features auf den Testdaten wie auf den Trainingsdaten zu haben\n",
    "    test_daten = test_data.drop(['Id'], axis = 1)\n",
    "    #füge eine Dimension hinzu, um prediction zu ermöglichen\n",
    "    test_daten = test_daten.values\n",
    "    test_daten = test_daten.reshape((test_daten.shape[0], 1, test_daten.shape[1]))\n",
    "    #Predicte das Ergebnis\n",
    "    predicted = model.predict_proba(test_daten)\n",
    "\n",
    "    \n",
    "    result = pd.DataFrame(predicted, columns = labels)\n",
    "    result.to_csv('submit_NN.csv',index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Zusätzliche Datapreparation, um es im Neuronalen Netz nutzen zu können\n",
    "#Nur einmal Ausführen\n",
    "y_train = to_categorical(y_train)\n",
    "x_train = x_train.values\n",
    "\n",
    "y_test = to_categorical(y_test)\n",
    "x_test = x_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Erstellung des tiefen neuronalen Netzes\n",
    "def create_NeuralNet(layer, nodes):\n",
    "  \n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(nodes, input_shape= x_train[0].shape, init='glorot_uniform', use_bias=True))\n",
    "    model_nn.add(PReLU())\n",
    "    model_nn.add(Dropout(0.2))\n",
    "  \n",
    "    for i in range(layer):\n",
    "        model_nn.add(Dense(nodes, init='glorot_uniform', use_bias=True))\n",
    "        model_nn.add(PReLU())\n",
    "        model_nn.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model_nn.add(Dense(39, activation='softmax'))\n",
    "\n",
    "\n",
    "    model_nn.compile(loss='categorical_crossentropy', optimizer=Adadelta(),  metrics=['acc'])\n",
    "  \n",
    "    return model_nn\n",
    "\n",
    "model_nn = create_NeuralNet(30, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#options\n",
    "history = model_nn.fit(x_train, \n",
    "          y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=512,\n",
    "          epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_nn.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model_nn.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('NN_Acc.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('NN_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model_nn.predict(x_test)\n",
    "np.argmax(y_pred, axis=1) \n",
    "pred_Test_NN = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.arange(1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Es wurde auf die Confusion Matrix für das Neuronale Netz verzichtet, da die Outputs keine boolean Matrix ist. Zur Richtigstellung muss ein Vektor von Label geschaffen werden, dass aber die CM verfälscht. \n",
    "\n",
    "y_pred = model_nn.predict(x_test)\n",
    "np.argmax(y_pred, axis=1) \n",
    "pred_Test_NN = (y_pred > 0.5)\n",
    "\n",
    "def plot_confusion_matrix(y_test, pred_Test_NN, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test.argmax(axis=1),pred_Test_NN.argmax(axis=1))\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_test, pred_Test_NN)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(20,15))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, pred_Test_NN, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.savefig('NN.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Zusätzliche Datapreparation, um es im LSTM Netz nutzen zu können\n",
    "x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_LSTM(layer, nodes):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(nodes,go_backwards= True, use_bias=True, kernel_initializer='glorot_uniform', input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "  \n",
    "\n",
    "    for i in range(layer):\n",
    "        model.add(LSTM(nodes, use_bias=True, kernel_initializer='glorot_uniform', input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "        model.add(PReLU())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(LSTM(39, use_bias=True, kernel_initializer='glorot_uniform', input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=False))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(39, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adadelta(), metrics=['acc'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = create_LSTM(50, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#options\n",
    "history_lstm = model.fit(x_train, \n",
    "          y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=512,\n",
    "          epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lstm = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_lstm[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn.save(\"model_nn.h5\")\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history_lstm.history['acc'])\n",
    "#plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('LSTM_Acc.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_lstm.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('LSTM_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausführen, um die Prediciton als CSV (submit.csv) zu speichern\n",
    "modelToCsv_deeplearning(model_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausführen, um die Prediciton als CSV (submit.csv) zu speichern\n",
    "modelToCsv_deeplearning(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
